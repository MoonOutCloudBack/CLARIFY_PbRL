{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import d4rl # Import required to register environments\n",
    "import deepdish as dd\n",
    "import os\n",
    "import wandb\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output, HTML, Image, display\n",
    "\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "sys.path.append('./data')\n",
    "sys.path.append('./..')\n",
    "sys.path.append('./../human_exp')\n",
    "sys.path.append('./../data')\n",
    "sys.path.append('./../..')\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "from reward_model import RewardModel\n",
    "from reward_model import remove_outliers, compute_distribution, compute_comparable_distribution_in_bins, reject_sampling\n",
    "from decision_transformer.models.decision_transformer import BidirectionalTransformer\n",
    "from decision_transformer.training.contrastive_trainer import ContrativeTrainer\n",
    "from env_wrapper import create_env, create_dataset, set_seed_everywhere, discount_cumsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reward model for human labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModelHuman(RewardModel):\n",
    "    def __init__(self, ds, da, device, trajectories, max_ep_len=500,\n",
    "                 ensemble_size=3, lr=3e-4, mb_size=128, size_segment=1, activation='tanh',\n",
    "                 capacity=5e5, large_batch=10,\n",
    "                 teacher_beta=-1, teacher_gamma=1, \n",
    "                 teacher_eps_mistake=0, teacher_eps_skip=0, teacher_eps_equal=0,\n",
    "                 use_skip_label=False, video_path='./human_exp/video_walker-walk/', \n",
    "                 segment_name_list=[], segment_traj_dict={},\n",
    "                 ):\n",
    "        super().__init__(ds, da, device, trajectories, max_ep_len=max_ep_len,\n",
    "            ensemble_size=ensemble_size, lr=lr, mb_size=mb_size, size_segment=size_segment, activation=activation,\n",
    "            capacity=capacity, large_batch=large_batch,\n",
    "            teacher_beta=teacher_beta, teacher_gamma=teacher_gamma, \n",
    "            teacher_eps_mistake=teacher_eps_mistake, teacher_eps_skip=teacher_eps_skip, teacher_eps_equal=teacher_eps_equal,\n",
    "            use_skip_label=use_skip_label,\n",
    "        )\n",
    "        self.get_label = self.get_onehot_label\n",
    "        self.video_path = video_path\n",
    "        self.segment_name_list = segment_name_list\n",
    "        self.segment_traj_dict = segment_traj_dict\n",
    "        self.segment_num = len(segment_name_list)\n",
    "        print(self.segment_num)\n",
    "\n",
    "        self.result_df = pd.DataFrame(columns=['query_idx_0', 'query_idx_1', 'return_0', 'return_1', 'rd', 'gt_label', 'human_label'])\n",
    "\n",
    "    \n",
    "    def get_queries(self, mb_size=20):\n",
    "        batch_index_1 = np.random.choice(self.segment_num, size=mb_size, replace=True)\n",
    "        batch_index_2 = np.random.choice(self.segment_num, size=mb_size, replace=True)\n",
    "        states_1 = np.zeros((mb_size, self.size_segment, self.ds), dtype=np.float32)\n",
    "        states_2 = np.zeros((mb_size, self.size_segment, self.ds), dtype=np.float32)\n",
    "        actions_1 = np.zeros((mb_size, self.size_segment, self.da), dtype=np.float32)\n",
    "        actions_2 = np.zeros((mb_size, self.size_segment, self.da), dtype=np.float32)\n",
    "        rewards_1 = np.zeros((mb_size, self.size_segment, 1), dtype=np.float32)\n",
    "        rewards_2 = np.zeros((mb_size, self.size_segment, 1), dtype=np.float32)\n",
    "\n",
    "        for i in range(mb_size):\n",
    "            # print(f'i = {i}')\n",
    "            # print(self.segment_name_list[batch_index_1[i]])\n",
    "            # print(self.segment_traj_dict[self.segment_name_list[batch_index_1[i]]])\n",
    "            states_1[i] = np.array(self.segment_traj_dict[self.segment_name_list\n",
    "                            [batch_index_1[i]]]['observations']).reshape(-1, self.ds)\n",
    "            actions_1[i] = np.array(self.segment_traj_dict[self.segment_name_list\n",
    "                            [batch_index_1[i]]]['actions']).reshape(-1, self.da)\n",
    "            rewards_1[i] = np.array(self.segment_traj_dict[self.segment_name_list\n",
    "                            [batch_index_1[i]]]['rewards']).reshape(-1, 1)\n",
    "            states_2[i] = np.array(self.segment_traj_dict[self.segment_name_list\n",
    "                            [batch_index_2[i]]]['observations']).reshape(-1, self.ds)\n",
    "            actions_2[i] = np.array(self.segment_traj_dict[self.segment_name_list\n",
    "                            [batch_index_2[i]]]['actions']).reshape(-1, self.da)\n",
    "            rewards_2[i] = np.array(self.segment_traj_dict[self.segment_name_list\n",
    "                            [batch_index_2[i]]]['rewards']).reshape(-1, 1)\n",
    "        \n",
    "        sa_t_1 = np.concatenate([states_1, actions_1], axis=2)  # Batch x size_seg x dim of s&a\n",
    "        sa_t_2 = np.concatenate([states_2, actions_2], axis=2)\n",
    "        r_t_1, r_t_2 = rewards_1, rewards_2  # Batch x size_seg x 1\n",
    "        tr_1, tr_2 = np.zeros((mb_size, 1)), np.zeros((mb_size, 1))\n",
    "        segment_name_list_0 = [self.segment_name_list[i] for i in batch_index_1]\n",
    "        segment_name_list_1 = [self.segment_name_list[i] for i in batch_index_2]\n",
    "        return sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, segment_name_list_0, segment_name_list_1\n",
    "\n",
    "\n",
    "    def get_onehot_label(self, sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, \n",
    "                         segment_name_list_0, segment_name_list_1, use_skip=True):\n",
    "        '''\n",
    "        Labeling rules and prompts are the same as verify_teacher.ipynb\n",
    "        '''\n",
    "        # print(segment_name_list_0)\n",
    "        total_label_num = len(segment_name_list_0)\n",
    "        label_results = []\n",
    "        gt_labels = np.zeros((total_label_num, 2))\n",
    "\n",
    "        for i in range(total_label_num):\n",
    "            # 给 self.result_df 新写一行\n",
    "            return_0, return_1 = np.sum(r_t_1[i]), np.sum(r_t_2[i])\n",
    "            gt_labels[i] = np.array([1.0, 0]) if return_0 > return_1 else np.array([0, 1.0])\n",
    "            if use_skip:\n",
    "                self.result_df.loc[len(self.result_df)] = {\n",
    "                    'query_idx_0': segment_name_list_0[i], \n",
    "                    'query_idx_1': segment_name_list_1[i], \n",
    "                    'return_0': return_0, \n",
    "                    'return_1': return_1, \n",
    "                    'rd': np.abs(return_0 - return_1), \n",
    "                    'gt_label': \"1 0\" if return_0 > return_1 else \"0 1\", \n",
    "                    'human_label': \"\"\n",
    "                }\n",
    "\n",
    "                clear_output()\n",
    "                print(f\"{i+1}th among total 20 feedbacks\")\n",
    "                query_idx_0 = segment_name_list_0[i]\n",
    "                query_idx_1 = segment_name_list_1[i]\n",
    "                print(f'query_idx_0: {query_idx_0}, query_idx_1: {query_idx_1}')\n",
    "\n",
    "                segment_0 = os.path.join(self.video_path, f'{query_idx_0}.gif')\n",
    "                segment_1 = os.path.join(self.video_path, f'{query_idx_1}.gif')\n",
    "                print(f'segment_0: {segment_0}')\n",
    "                print(f'segment_1: {segment_1}')\n",
    "\n",
    "                time.sleep(0.1)\n",
    "                display(HTML(f'''\n",
    "                <div style=\"display: inline-block; margin-right: 100px;\">\n",
    "                    <img src=\"{segment_0}\" width=\"400\" loop=\"true\" >\n",
    "                </div>\n",
    "                '''))\n",
    "                time.sleep(0.1)\n",
    "                display(HTML(f'''\n",
    "                <div style=\"display: inline-block;\">\n",
    "                    <img src=\"{segment_1}\" width=\"400\" loop=\"true\" >\n",
    "                </div>\n",
    "                '''))\n",
    "                time.sleep(0.1)\n",
    "\n",
    "                select = input('select')\n",
    "                if select == 'quit' or select == 'exit':\n",
    "                    break\n",
    "                self.result_df.loc[len(self.result_df) - 1, 'human_label'] = select\n",
    "                label_results.append(select)\n",
    "\n",
    "                self.result_df.to_csv('./human_label_results.csv', index=False)\n",
    "        \n",
    "        if not use_skip:\n",
    "            return sa_t_1, sa_t_2, r_t_1, r_t_2, gt_labels, {}\n",
    "\n",
    "        labels = np.array([[1, 0] if '1 0' in lbl else ([0, 1] if '0 1' in lbl else [0.5, 0.5]) \n",
    "                  for lbl in label_results])\n",
    "        labels_undist_count = np.sum(np.all(labels == np.array([0.5, 0.5]), axis=1))\n",
    "        labels_mismatch_count = np.sum(np.all(labels != gt_labels, axis=1))\n",
    "        metrics = {\n",
    "            'skip_ratio': labels_undist_count / total_label_num,\n",
    "            'error_ratio': (labels_mismatch_count - labels_undist_count) / total_label_num\n",
    "        }\n",
    "        return sa_t_1, sa_t_2, r_t_1, r_t_2, labels, metrics\n",
    "\n",
    "\n",
    "    def uniform_sampling(self):\n",
    "        # get queries\n",
    "        sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, segment_name_list_0, segment_name_list_1 = \\\n",
    "            self.get_queries(mb_size=self.mb_size)\n",
    "\n",
    "        # get labels\n",
    "        sa_t_1, sa_t_2, r_t_1, r_t_2, labels, metrics = self.get_label(\n",
    "            sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, segment_name_list_0, segment_name_list_1)\n",
    "        if len(labels) > 0:\n",
    "            self.put_queries(sa_t_1, sa_t_2, r_t_1, r_t_2, labels)\n",
    "        \n",
    "        return len(labels), metrics\n",
    "    \n",
    "\n",
    "    def disagreement_sampling(self):\n",
    "        # get queries\n",
    "        sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, segment_name_list_0, segment_name_list_1 = \\\n",
    "            self.get_queries(mb_size=self.mb_size*self.large_batch)\n",
    "        \n",
    "        # get final queries based on uncertainty\n",
    "        _, disagree = self.get_rank_probability(sa_t_1, sa_t_2)\n",
    "        top_k_index = (-disagree).argsort()[:self.mb_size]\n",
    "        r_t_1, sa_t_1, tr_1 = r_t_1[top_k_index], sa_t_1[top_k_index], tr_1[top_k_index]\n",
    "        r_t_2, sa_t_2, tr_2 = r_t_2[top_k_index], sa_t_2[top_k_index], tr_2[top_k_index]\n",
    "        segment_name_list_0 = [segment_name_list_0[i] for i in top_k_index]\n",
    "        segment_name_list_1 = [segment_name_list_1[i] for i in top_k_index]\n",
    "        \n",
    "        # get labels\n",
    "        sa_t_1, sa_t_2, r_t_1, r_t_2, labels, metrics = self.get_label(\n",
    "            sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, segment_name_list_0, segment_name_list_1)        \n",
    "        if len(labels) > 0:\n",
    "            self.put_queries(sa_t_1, sa_t_2, r_t_1, r_t_2, labels)\n",
    "        \n",
    "        return len(labels), metrics\n",
    "\n",
    "\n",
    "    def contrastive_sampling(self):\n",
    "        distances, labels = self.sample_learned_queries(self.mb_size * self.large_batch)\n",
    "        idx = remove_outliers(distances)  # remove outliers\n",
    "        distances, labels = distances[idx], labels[idx]\n",
    "        bin_centers, hist = compute_distribution(distances, num_discrete=self.num_discrete)\n",
    "\n",
    "        comparable = (labels[:, 0] != 0.5)\n",
    "        # comparable\n",
    "        dist_comparable = distances[comparable]\n",
    "        hist_comparable = compute_comparable_distribution_in_bins(\n",
    "            dist_comparable, bin_centers, num_discrete=self.num_discrete)\n",
    "        hist_comparable_density = hist_comparable / (hist + 1e-6)\n",
    "        hist_comparable_density /= np.sum(hist_comparable_density)\n",
    "        # non-comparable\n",
    "        dist_non_comparable = distances[~comparable]\n",
    "        hist_non_comparable = compute_comparable_distribution_in_bins(\n",
    "            dist_non_comparable, bin_centers, num_discrete=self.num_discrete)\n",
    "        hist_non_comparable_density = hist_non_comparable / (hist + 1e-6)\n",
    "        hist_non_comparable_density /= np.sum(hist_non_comparable_density)\n",
    "\n",
    "        # get queries\n",
    "        sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, segment_name_list_0, segment_name_list_1 = \\\n",
    "            self.get_queries(mb_size=self.mb_size * self.large_batch)\n",
    "        new_distances = self.convert_queries_to_embeddings(sa_t_1, sa_t_2)\n",
    "        idx = remove_outliers(new_distances)  # remove outliers\n",
    "        sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, new_distances = \\\n",
    "            sa_t_1[idx], sa_t_2[idx], r_t_1[idx], r_t_2[idx], tr_1[idx], tr_2[idx], new_distances[idx]\n",
    "        segment_name_list_0 = [segment_name_list_0[i] for i in idx]\n",
    "        segment_name_list_1 = [segment_name_list_1[i] for i in idx]\n",
    "        \n",
    "        new_hist = compute_comparable_distribution_in_bins(\n",
    "            new_distances, bin_centers, num_discrete=self.num_discrete)\n",
    "        target_hist_minus = np.clip((hist_comparable_density - hist_non_comparable_density), 0, None) * hist\n",
    "        target_hist_minus /= np.sum(target_hist_minus)\n",
    "        target_hist_div = np.clip((hist_comparable_density / (hist_non_comparable_density + 1e-6)), 0, None) * hist\n",
    "        target_hist_div /= np.sum(target_hist_div)\n",
    "        target_hist = (target_hist_minus + target_hist_div) * 0.5\n",
    "\n",
    "        # reject sampling\n",
    "        idx = reject_sampling(new_distances, new_hist, target_hist, bin_centers, \n",
    "                              num_discrete=self.num_discrete)\n",
    "        sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, new_distances = \\\n",
    "            sa_t_1[idx], sa_t_2[idx], r_t_1[idx], r_t_2[idx], tr_1[idx], tr_2[idx], new_distances[idx]\n",
    "        segment_name_list_0 = [segment_name_list_0[i] for i in idx]\n",
    "        segment_name_list_1 = [segment_name_list_1[i] for i in idx]\n",
    "\n",
    "        # get final queries based on uncertainty\n",
    "        _, disagree = self.get_rank_probability(sa_t_1, sa_t_2)\n",
    "        top_k_index = (-disagree).argsort()[:self.mb_size]\n",
    "        r_t_1, sa_t_1, tr_1 = r_t_1[top_k_index], sa_t_1[top_k_index], tr_1[top_k_index]\n",
    "        r_t_2, sa_t_2, tr_2 = r_t_2[top_k_index], sa_t_2[top_k_index], tr_2[top_k_index]\n",
    "        segment_name_list_0 = [segment_name_list_0[i] for i in top_k_index]\n",
    "        segment_name_list_1 = [segment_name_list_1[i] for i in top_k_index]\n",
    "        \n",
    "        # get labels\n",
    "        sa_t_1, sa_t_2, r_t_1, r_t_2, labels, metrics = self.get_label(\n",
    "            sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, segment_name_list_0, segment_name_list_1)\n",
    "        if len(labels) > 0:\n",
    "            self.put_queries(sa_t_1, sa_t_2, r_t_1, r_t_2, labels)\n",
    "        \n",
    "        return len(labels), metrics\n",
    "    \n",
    "\n",
    "    def test_reward_model_accuracy(self):\n",
    "        ''' test the reward model on the test set '''\n",
    "        sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, segment_name_list_0, segment_name_list_1 = \\\n",
    "            self.get_queries(self.train_batch_size)\n",
    "        sa_t_1, sa_t_2, r_t_1, r_t_2, labels, _ = self.get_label(\n",
    "            sa_t_1, sa_t_2, r_t_1, r_t_2, tr_1, tr_2, \n",
    "            segment_name_list_0, segment_name_list_1, use_skip=False)\n",
    "        ensemble_acc = np.array([0 for _ in range(self.de)])\n",
    "        total = r_t_1.shape[0]\n",
    "        if isinstance(labels, np.ndarray):\n",
    "            labels = torch.from_numpy(labels).float().to(self.device)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "        for member in range(self.de):\n",
    "            # get logits\n",
    "            r_hat1 = self.r_hat_member(sa_t_1, member=member)\n",
    "            r_hat2 = self.r_hat_member(sa_t_2, member=member)\n",
    "            r_hat1 = r_hat1.sum(axis=1)\n",
    "            r_hat2 = r_hat2.sum(axis=1)\n",
    "            r_hat = torch.cat([r_hat1, r_hat2], axis=-1)\n",
    "\n",
    "            # compute acc\n",
    "            _, predicted = torch.max(r_hat.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            ensemble_acc[member] += correct\n",
    "\n",
    "        ensemble_acc = ensemble_acc / total\n",
    "\n",
    "        return np.mean(ensemble_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--env', type=str, default='walker-walk')\n",
    "parser.add_argument('--dataset', type=str, default='medium-expert')\n",
    "parser.add_argument('--K', type=int, default=50)  # segment size & context size, original 20\n",
    "parser.add_argument('--segment_length', type=int, default=50)  # should be same as K\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "# contrastive model\n",
    "parser.add_argument('--use_contrastive', type=bool, default=True)  # if True, only train reward model\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--embed_dim', type=int, default=256)  # original 128\n",
    "parser.add_argument('--n_layer', type=int, default=4)  # original 3\n",
    "parser.add_argument('--n_head', type=int, default=4)  # original 1\n",
    "parser.add_argument('--activation_function', type=str, default='relu')\n",
    "parser.add_argument('--dropout', type=float, default=0.1)\n",
    "parser.add_argument('--learning_rate', '-lr', type=float, default=1e-4)\n",
    "parser.add_argument('--z_star_lr', type=float, default=1e-3)\n",
    "parser.add_argument('--weight_decay', '-wd', type=float, default=1e-4)\n",
    "parser.add_argument('--warmup_steps', type=int, default=10000)\n",
    "parser.add_argument('--num_steps_per_iter', type=int, default=500)  # original 2000\n",
    "parser.add_argument('--num_steps_init', type=int, default=5000)  # 10 iters\n",
    "# parser.add_argument('--num_steps_per_iter', type=int, default=2)  # debug\n",
    "# parser.add_argument('--num_steps_init', type=int, default=2)  # 10 iters  # debug\n",
    "parser.add_argument('--dist_dim', type=int, default=30)\n",
    "parser.add_argument('--n_bins', type=int, default=31)\n",
    "parser.add_argument('--gamma', type=float, default=1.00)\n",
    "parser.add_argument('--save_model', type=bool, default=True)\n",
    "parser.add_argument('--z_dim', type=int, default=16)  # original 1\n",
    "# PbHIM\n",
    "parser.add_argument('--similarity_fn', type=str, default='l2')\n",
    "parser.add_argument('--norm_loss_ratio', type=float, default=0.1)\n",
    "parser.add_argument('--comp_loss_ratio', type=float, default=0.1)\n",
    "parser.add_argument('--pref_loss_ratio', type=float, default=1.0)\n",
    "parser.add_argument('--pref_loss_impl', type=str, default='pairwise')\n",
    "# for eval\n",
    "parser.add_argument('--num_eval_episodes', type=int, default=20)\n",
    "# parser.add_argument('--num_eval_episodes', type=int, default=2)  # debug\n",
    "parser.add_argument('--save_rollout', type=bool, default=False)\n",
    "parser.add_argument('--output_dir', type=str, default='./../results')\n",
    "\n",
    "# reward learning\n",
    "parser.add_argument('--reward_batch', type=int, default=20)\n",
    "parser.add_argument('--max_feedback', type=int, default=100)\n",
    "parser.add_argument('--feed_type', type=str, default='c')  # d, c (contrastive)\n",
    "parser.add_argument('--train_num_iter', type=int, default=50)\n",
    "parser.add_argument('--reward_lr', type=float, default=3e-4)\n",
    "parser.add_argument('--reward_update', type=int, default=50)\n",
    "\n",
    "# dummy\n",
    "parser.add_argument('--dummy1', type=str, default='d')  # d, c (contrastive)\n",
    "parser.add_argument('--dummy2', type=int, default=50)  # d, c (contrastive)\n",
    "parser.add_argument('--dummy3', type=float, default=3e-4)  # d, c (contrastive)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "print(args)\n",
    "\n",
    "# random seed\n",
    "set_seed_everywhere(args.seed)\n",
    "\n",
    "# log dir\n",
    "use_contrastive = args.use_contrastive\n",
    "if use_contrastive:\n",
    "    save_dir = f'reward-{args.env}-{args.dataset}-conbdt'\n",
    "    save_dir += f'-norm-{args.norm_loss_ratio}-comp-{args.comp_loss_ratio}-pref-{args.pref_loss_ratio}'\n",
    "    save_dir += f'_fb_{args.max_feedback}_q_{args.reward_batch}_human_{args.feed_type}'\n",
    "    save_dir += f'-ctx_{args.K}-seed_{args.seed}-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "else:\n",
    "    save_dir = f'reward-{args.env}-{args.dataset}-nocon'\n",
    "    save_dir += f'_fb_{args.max_feedback}_q_{args.reward_batch}_human_{args.feed_type}'\n",
    "    save_dir += f'-ctx_{args.K}-seed_{args.seed}-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "output_dir = os.path.join(args.output_dir, save_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "eval_dir = os.path.join(output_dir, f'eval')\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'params.json'), mode=\"w\") as f:\n",
    "    json.dump(args.__dict__, f, indent=4)\n",
    "\n",
    "variant=vars(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. build env and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = variant.get('gpu', 0)\n",
    "device = torch.device(\n",
    "    f\"cuda:{gpu}\" if (torch.cuda.is_available() and gpu >= 0) else \"cpu\"\n",
    ")\n",
    "\n",
    "env_name, dataset = variant['env'], variant['dataset']\n",
    "seed = variant['seed']\n",
    "n_bins = variant['n_bins']\n",
    "use_contrastive = variant['use_contrastive']\n",
    "print(f'use_contrastive: {use_contrastive}')\n",
    "gamma = variant['gamma']\n",
    "assert gamma == 1.\n",
    "z_dim = variant['z_dim']\n",
    "\n",
    "env, eval_env, scale, max_ep_len = create_env(env_name, seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "trajectories, states, traj_lens, returns, rewards = create_dataset(\n",
    "    env_name, dataset, using_notebook=True)\n",
    "\n",
    "# used for input normalization\n",
    "states = np.concatenate(states, axis=0)\n",
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "num_timesteps = sum(traj_lens)\n",
    "\n",
    "print('=' * 50)\n",
    "print(f'Starting new experiment: {env_name} {dataset}')\n",
    "print(f'{len(traj_lens)} trajectories, {num_timesteps} timesteps found')\n",
    "print(f'Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}')\n",
    "print(f'Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}')\n",
    "print(f'z-dim: {z_dim}')\n",
    "print('=' * 50)\n",
    "\n",
    "K = variant['K']\n",
    "batch_size = variant['batch_size']\n",
    "\n",
    "# for evaluation with best/50% trajectories\n",
    "_idxes = np.argsort([np.sum(path['rewards']) for path in trajectories]) # rank 0 is the most bad demo.\n",
    "trajs_rank = np.empty_like(_idxes)\n",
    "trajs_rank[_idxes] = np.arange(len(_idxes))\n",
    "train_indices =  [i for i in range(len(trajs_rank))]\n",
    "\n",
    "\n",
    "def get_batch(batch_size=256, max_len=K):\n",
    "    batch_inds = np.random.choice(\n",
    "        np.array(train_indices),\n",
    "        size=batch_size,\n",
    "        replace=True,\n",
    "    )\n",
    "    s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "    for i in range(batch_size):\n",
    "        traj = trajectories[int(batch_inds[i])]\n",
    "        si = random.randint(0, traj['rewards'].shape[0] - 1)\n",
    "\n",
    "        s.append(traj['observations'][si:si + max_len].reshape(1, -1, state_dim))\n",
    "        a.append(traj['actions'][si:si + max_len].reshape(1, -1, act_dim))\n",
    "        r.append(traj['rewards'][si:si + max_len].reshape(1, -1, 1))\n",
    "        if 'terminals' in traj:\n",
    "            d.append(traj['terminals'][si:si + max_len].reshape(1, -1))\n",
    "        else:\n",
    "            d.append(traj['dones'][si:si + max_len].reshape(1, -1))\n",
    "        timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "        timesteps[-1][timesteps[-1] >= max_ep_len] = max_ep_len-1  # padding cutoff\n",
    "        rtg.append(discount_cumsum(traj['rewards'][si:], gamma=1.)[:s[-1].shape[1] + 1].reshape(1, -1, 1))\n",
    "        if rtg[-1].shape[1] <= s[-1].shape[1]:\n",
    "            rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "        tlen = s[-1].shape[1]\n",
    "        s[-1] = np.concatenate([np.zeros((1, max_len - tlen, state_dim)), s[-1]], axis=1)\n",
    "        s[-1] = (s[-1] - state_mean) / state_std\n",
    "        a[-1] = np.concatenate([np.ones((1, max_len - tlen, act_dim)) * -10., a[-1]], axis=1)\n",
    "        r[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), r[-1]], axis=1)\n",
    "        d[-1] = np.concatenate([np.ones((1, max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "        rtg[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), rtg[-1]], axis=1)\n",
    "        timesteps[-1] = np.concatenate([np.zeros((1, max_len - tlen)), timesteps[-1]], axis=1)\n",
    "        mask.append(np.concatenate([np.zeros((1, max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "    s = torch.from_numpy(np.concatenate(s, axis=0)).to(dtype=torch.float32, device=device)  # (B, K, state_dim)\n",
    "    a = torch.from_numpy(np.concatenate(a, axis=0)).to(dtype=torch.float32, device=device)\n",
    "    r = torch.from_numpy(np.concatenate(r, axis=0)).to(dtype=torch.float32, device=device)  # (B, K, 1)\n",
    "    d = torch.from_numpy(np.concatenate(d, axis=0)).to(dtype=torch.long, device=device)\n",
    "    rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).to(dtype=torch.float32, device=device) / scale  # (B, K, 1)\n",
    "    timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).to(dtype=torch.long, device=device)\n",
    "    mask = torch.from_numpy(np.concatenate(mask, axis=0)).to(device=device)\n",
    "\n",
    "    return s, a, r, d, rtg, timesteps, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. reward learning support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward learning config\n",
    "max_feedback = variant['max_feedback']\n",
    "total_feedback, labeled_feedback = 0, 0\n",
    "segment_length = variant['segment_length']\n",
    "reward_lr = variant['reward_lr']\n",
    "reward_batch = variant['reward_batch']\n",
    "reward_update = variant['reward_update']\n",
    "feed_type = variant['feed_type']\n",
    "\n",
    "# 读取 ./human_exp/video_{env_name} 下所有以 .gif 结尾的文件，组成一个 list\n",
    "video_path = f'./../human_exp/video_{env_name}/'\n",
    "print(\"video_path: \", video_path)\n",
    "segment_name_list = [f[:-4] for f in os.listdir(video_path) if f.endswith('.gif')]\n",
    "segment_traj_dict = {seg_name: pickle.load(open(os.path.join(\n",
    "    video_path, f'{seg_name}.pkl'), 'rb')) for seg_name in segment_name_list}\n",
    "segment_num = len(segment_name_list)\n",
    "print(\"segment_name_list: \", segment_name_list[:5])\n",
    "\n",
    "\n",
    "reward_model = RewardModelHuman(\n",
    "    ds=state_dim, da=act_dim, device=device, mb_size=reward_batch,\n",
    "    size_segment=segment_length, trajectories=trajectories,\n",
    "    max_ep_len=max_ep_len, teacher_eps_skip=0.0, lr=reward_lr,\n",
    "    video_path=video_path, segment_name_list=segment_name_list, segment_traj_dict=segment_traj_dict)\n",
    "\n",
    "\n",
    "def get_pref_batch(batch_size=256, max_len=K, reward_model=reward_model):\n",
    "    max_pref_len = reward_model.capacity if reward_model.buffer_full else reward_model.buffer_index\n",
    "    batch_inds = np.random.choice(np.arange(max_pref_len), size=batch_size, replace=True)\n",
    "    sa_t_1 = reward_model.buffer_seg1[batch_inds]\n",
    "    sa_t_2 = reward_model.buffer_seg2[batch_inds]\n",
    "    labels = reward_model.buffer_label[batch_inds]\n",
    "    states_1, actions_1 = sa_t_1[:, :, :state_dim], sa_t_1[:, :, state_dim:]\n",
    "    states_2, actions_2 = sa_t_2[:, :, :state_dim], sa_t_2[:, :, state_dim:]\n",
    "    rewards_1 = reward_model.buffer_reward1[batch_inds]\n",
    "    rewards_2 = reward_model.buffer_reward2[batch_inds]\n",
    "    dones_1 = dones_2 = torch.zeros((batch_size, max_len), dtype=torch.long, device=device)\n",
    "    attention_mask_1 = torch.ones((batch_size, max_len), dtype=torch.float32, device=device)\n",
    "    attention_mask_2 = torch.ones((batch_size, max_len), dtype=torch.float32, device=device)\n",
    "    rtg_1 = rtg_2 = torch.ones((batch_size, max_len, 1), dtype=torch.float32, device=device)\n",
    "    random_time_start = np.random.randint(0, max_ep_len - max_len - 1, size=(batch_size, 2))\n",
    "    timesteps_1 = np.arange(max_len).reshape(1, -1) + random_time_start[:, 0].reshape(-1, 1)\n",
    "    timesteps_2 = np.arange(max_len).reshape(1, -1) + random_time_start[:, 1].reshape(-1, 1)\n",
    "\n",
    "    # to tensor\n",
    "    states_1 = torch.from_numpy(states_1).to(dtype=torch.float32, device=device)\n",
    "    actions_1 = torch.from_numpy(actions_1).to(dtype=torch.float32, device=device)\n",
    "    rewards_1 = torch.from_numpy(rewards_1).to(dtype=torch.float32, device=device)\n",
    "    timesteps_1 = torch.from_numpy(timesteps_1).to(dtype=torch.long, device=device)\n",
    "    states_2 = torch.from_numpy(states_2).to(dtype=torch.float32, device=device)\n",
    "    actions_2 = torch.from_numpy(actions_2).to(dtype=torch.float32, device=device)\n",
    "    rewards_2 = torch.from_numpy(rewards_2).to(dtype=torch.float32, device=device)\n",
    "    timesteps_2 = torch.from_numpy(timesteps_2).to(dtype=torch.long, device=device)\n",
    "    labels = torch.from_numpy(labels).to(dtype=torch.float32, device=device)\n",
    "    return states_1, actions_1, rewards_1, dones_1, rtg_1, timesteps_1, attention_mask_1, \\\n",
    "            states_2, actions_2, rewards_2, dones_2, rtg_2, timesteps_2, attention_mask_2, \\\n",
    "            labels\n",
    "\n",
    "\n",
    "def learn_reward(total_feedback, labeled_feedback, first_flag=0):\n",
    "    print(f'total_feedback = {total_feedback}, labeled_feedback = {labeled_feedback}')\n",
    "    # get feedbacks\n",
    "    if first_flag == 1:\n",
    "        # if it is first time to get feedback, need to use random sampling\n",
    "        labeled_queries, metrics = reward_model.uniform_sampling()\n",
    "    else:\n",
    "        if feed_type == 0 or feed_type == 'u':\n",
    "            labeled_queries, metrics = reward_model.uniform_sampling()\n",
    "        elif feed_type == 1 or feed_type == 'd':\n",
    "            labeled_queries, metrics = reward_model.disagreement_sampling()\n",
    "        elif feed_type == 2 or feed_type == 'c':\n",
    "            labeled_queries, metrics = reward_model.contrastive_sampling()\n",
    "        elif feed_type == 20 or feed_type == 'cm':\n",
    "            labeled_queries, metrics = reward_model.max_ratio_sampling()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    # nonlocal total_feedback, labeled_feedback\n",
    "    total_feedback += reward_model.mb_size\n",
    "    labeled_feedback += labeled_queries\n",
    "    \n",
    "    train_acc = 0\n",
    "    if labeled_feedback > 0:\n",
    "        # update reward\n",
    "        for epoch in range(reward_update):\n",
    "            train_acc, reward_loss = reward_model.train_reward()\n",
    "            train_acc = np.mean(train_acc)\n",
    "            if train_acc > 0.97:\n",
    "                break\n",
    "    eval_acc = reward_model.test_reward_model_accuracy()\n",
    "    print(f\"Reward function is updated!! train ACC: {train_acc:.2f}, eval ACC: {eval_acc:.2f}, epoch: {epoch}\")\n",
    "\n",
    "    metrics['reward_train_acc'] = train_acc\n",
    "    metrics['reward_eval_acc'] = eval_acc\n",
    "    metrics['reward_loss'] = reward_loss\n",
    "    return metrics, total_feedback, labeled_feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the contrastive model\n",
    "model = BidirectionalTransformer(\n",
    "    state_dim=state_dim,\n",
    "    act_dim=act_dim,\n",
    "    hidden_size=variant['embed_dim'],\n",
    "    z_dim=z_dim,\n",
    "    max_length=K,\n",
    "    max_ep_len=max_ep_len,\n",
    "    # transformer parameters\n",
    "    n_layer=variant['n_layer'],\n",
    "    n_head=variant['n_head'],\n",
    "    n_inner=4*variant['embed_dim'],\n",
    "    activation_function=variant['activation_function'],\n",
    "    n_positions=1024,\n",
    "    resid_pdrop=variant['dropout'],\n",
    "    attn_pdrop=variant['dropout'],\n",
    ")\n",
    "model = model.to(device=device)\n",
    "warmup_steps = variant['warmup_steps']\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=variant['learning_rate'],\n",
    "    weight_decay=variant['weight_decay'],\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lambda steps: min((steps+1)/warmup_steps, 1)\n",
    ")\n",
    "z_star = torch.nn.parameter.Parameter(torch.empty(z_dim, requires_grad=True, device=device))\n",
    "torch.nn.init.normal_(z_star)\n",
    "z_star_optimizer = torch.optim.AdamW(\n",
    "    [z_star],\n",
    "    lr=variant[\"z_star_lr\"],\n",
    "    weight_decay=variant['weight_decay']\n",
    ")\n",
    "reward_model.set_bdt_model(model)\n",
    "\n",
    "trainer = ContrativeTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=batch_size,\n",
    "    get_batch=get_batch,\n",
    "    get_pref_batch=get_pref_batch,\n",
    "    z_star=z_star,\n",
    "    z_star_optimizer=z_star_optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=lambda s_hat, a_hat, r_hat, s, a, r: torch.mean((a_hat - a)**2),\n",
    "    eval_fns=[],\n",
    "    eval_bdt_z_stars=[],\n",
    "    similarity_fn=variant['similarity_fn'], \n",
    "    norm_loss_ratio=variant['norm_loss_ratio'],\n",
    "    comp_loss_ratio=variant['comp_loss_ratio'],\n",
    "    pref_loss_ratio=variant['pref_loss_ratio'],\n",
    "    pref_loss_impl=variant['pref_loss_impl'], \n",
    "    device=device,\n",
    ")\n",
    "\n",
    "if use_contrastive:\n",
    "    name = f'conbdt_{env_name}-{dataset}'\n",
    "    name += f'_norm_{variant[\"norm_loss_ratio\"]}_comp_{variant[\"comp_loss_ratio\"]}_pref_{variant[\"pref_loss_ratio\"]}'\n",
    "    name += f'_fb_{max_feedback}_q_{reward_batch}_human_{feed_type}'\n",
    "    name += f'_ctx_{K}_seed_{seed}_{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "else:\n",
    "    name = f'reward_{env_name}-{dataset}_fb_{max_feedback}_q_{reward_batch}_human_{feed_type}'\n",
    "    name += f'_ctx_{K}_seed_{seed}_{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    assert feed_type != 'c'  # shouldn't use contrastive sampling\n",
    "wandb.init(project='clarify_reward', name=name, config=variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "itr = 0\n",
    "total_feedback, labeled_feedback = 0, 0\n",
    "while total_feedback < max_feedback:\n",
    "    reward_model.change_batch(1.0)\n",
    "    if reward_model.mb_size + total_feedback > max_feedback:\n",
    "        reward_model.set_batch(max_feedback - total_feedback)\n",
    "\n",
    "    metrics, total_feedback, labeled_feedback = learn_reward(total_feedback, labeled_feedback, \n",
    "                                                             first_flag=1 if itr == 0 else 0)\n",
    "    wandb.log(metrics, step=total_feedback)\n",
    "    if use_contrastive:\n",
    "        outputs = trainer.train_iteration(\n",
    "            num_steps=variant['num_steps_init'] if itr == 0 else variant['num_steps_per_iter'], iter_num=itr+1, print_logs=True)\n",
    "        wandb.log(outputs, step=total_feedback)\n",
    "    itr += 1\n",
    "\n",
    "    # save reward model, contrastive model\n",
    "    if total_feedback % 100 == 0:\n",
    "        reward_model.save_reward_model(os.path.join(output_dir, f'reward_{total_feedback}.pth'))\n",
    "        if use_contrastive:\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, f'dt_{total_feedback}.pth'))\n",
    "            # torch.save(z_star, os.path.join(output_dir, f'z_star_{total_feedback}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mn_pbhim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
